{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS82hd29uoU8"
      },
      "source": [
        "# Training a causal language model from scratch (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1A5eHRfuoVI"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBzo8q0CuoVK",
        "outputId": "43fa8441-033e-4da7-9197-6d68fa47739a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.7.1-py3-none-any.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.3.0-py3-none-any.whl (72 kB)\n",
            "\u001b[K     |████████████████████████████████| 72 kB 903 kB/s \n",
            "\u001b[?25hCollecting transformers[sentencepiece]\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 46.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.6)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 48.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.13.0)\n",
            "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
            "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 44.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 49.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 52.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 15.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<=3.20.2 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.19.6)\n",
            "Collecting sentencepiece!=0.1.92,>=0.1.91\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 13.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: urllib3, xxhash, tokenizers, responses, multiprocess, huggingface-hub, transformers, sentencepiece, datasets, evaluate\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.7.1 evaluate-0.3.0 huggingface-hub-0.11.0 multiprocess-0.70.14 responses-0.18.0 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.24.0 urllib3-1.25.11 xxhash-3.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.14.0-py3-none-any.whl (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.12.1+cu113)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (21.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.21.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from accelerate) (5.4.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->accelerate) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install accelerate\n",
        "# To run the training on TPU, you will need to uncomment the followin line:\n",
        "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsPgG5ZUuoVN"
      },
      "source": [
        "You will need to setup git, adapt your email and name in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbbsnLKcuoVO"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"you@example.com\"\n",
        "!git config --global user.name \"Your Name\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4cRS7bWuoVP"
      },
      "source": [
        "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ339FNxuoVQ"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YX0PFjiLuoVR"
      },
      "outputs": [],
      "source": [
        "def any_keyword_in_string(string, keywords):\n",
        "    for keyword in keywords:\n",
        "        if keyword in string:\n",
        "            return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-9QNyv-uoVS",
        "outputId": "560117a1-3128-4a0b-9bc6-b15af445d8ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False True"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
        "example_1 = \"import numpy as np\"\n",
        "example_2 = \"import pandas as pd\"\n",
        "\n",
        "print(\n",
        "    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Xhcsm-PuoVU"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset\n",
        "\n",
        "\n",
        "def filter_streaming_dataset(dataset, filters):\n",
        "    filtered_dict = defaultdict(list)\n",
        "    total = 0\n",
        "    for sample in tqdm(iter(dataset)):\n",
        "        total += 1\n",
        "        if any_keyword_in_string(sample[\"content\"], filters):\n",
        "            for k, v in sample.items():\n",
        "                filtered_dict[k].append(v)\n",
        "    print(f\"{len(filtered_dict['content'])/total:.2%} of data after filtering.\")\n",
        "    return Dataset.from_dict(filtered_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RiCV9i0uoVV",
        "outputId": "31fd6be2-df63-4d37-a149-cd60a117206d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3.26% of data after filtering."
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This cell will take a very long time to execute, so you should skip it and go to\n",
        "# the next one!\n",
        "from datasets import load_dataset\n",
        "\n",
        "split = \"train\"  # \"valid\"\n",
        "filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
        "\n",
        "data = load_dataset(f\"transformersbook/codeparrot-{split}\", split=split, streaming=True)\n",
        "filtered_data = filter_streaming_dataset(data, filters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PL-gx8JuoVX",
        "outputId": "6d7578d4-c097-4242-d0aa-dc0989ecb25b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
              "        num_rows: 606720\n",
              "    })\n",
              "    valid: Dataset({\n",
              "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
              "        num_rows: 3322\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
        "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
        "\n",
        "raw_datasets = DatasetDict(\n",
        "    {\n",
        "        \"train\": ds_train,  # .shuffle().select(range(50000)),\n",
        "        \"valid\": ds_valid,  # .shuffle().select(range(500))\n",
        "    }\n",
        ")\n",
        "\n",
        "raw_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmVNAihQuoVZ",
        "outputId": "94d8eaeb-90c7-4532-be8f-835b7ca84232"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'REPO_NAME: kmike/scikit-learn'\n",
              "'PATH: sklearn/utils/__init__.py'\n",
              "'COPIES: 3'\n",
              "'SIZE: 10094'\n",
              "'''CONTENT: \"\"\"\n",
              "The :mod:`sklearn.utils` module includes various utilites.\n",
              "\"\"\"\n",
              "\n",
              "from collections import Sequence\n",
              "\n",
              "import numpy as np\n",
              "from scipy.sparse import issparse\n",
              "import warnings\n",
              "\n",
              "from .murmurhash import murm\n",
              "LICENSE: bsd-3-clause'''"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for key in raw_datasets[\"train\"][0]:\n",
        "    print(f\"{key.upper()}: {raw_datasets['train'][0][key][:200]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-V4-jkBuoVa",
        "outputId": "4b5313c7-a4fa-42e7-d311-e234601026ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Input IDs length: 34\n",
              "Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]\n",
              "Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "context_length = 128\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
        "\n",
        "outputs = tokenizer(\n",
        "    raw_datasets[\"train\"][:2][\"content\"],\n",
        "    truncation=True,\n",
        "    max_length=context_length,\n",
        "    return_overflowing_tokens=True,\n",
        "    return_length=True,\n",
        ")\n",
        "\n",
        "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
        "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
        "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D05wJlyQuoVb",
        "outputId": "94d00427-9605-468e-ccf2-7297f760e98a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 16702061\n",
              "    })\n",
              "    valid: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 93164\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def tokenize(element):\n",
        "    outputs = tokenizer(\n",
        "        element[\"content\"],\n",
        "        truncation=True,\n",
        "        max_length=context_length,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_length=True,\n",
        "    )\n",
        "    input_batch = []\n",
        "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
        "        if length == context_length:\n",
        "            input_batch.append(input_ids)\n",
        "    return {\"input_ids\": input_batch}\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
        ")\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sO8QeWa0uoVd"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    \"gpt2\",\n",
        "    vocab_size=len(tokenizer),\n",
        "    n_ctx=context_length,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWckZQUCuoVf",
        "outputId": "f1909b5b-66b5-4c94-a29c-f1361d213625"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT-2 size: 124.2M parameters"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = GPT2LMHeadModel(config)\n",
        "model_size = sum(t.numel() for t in model.parameters())\n",
        "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00p_hJ3vuoVg"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMMjVpDyuoVh",
        "outputId": "fcdb2b31-0d5a-4009-c8d5-991c161c8077"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "input_ids shape: torch.Size([5, 128])\n",
              "attention_mask shape: torch.Size([5, 128])\n",
              "labels shape: torch.Size([5, 128])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n",
        "for key in out:\n",
        "    print(f\"{key} shape: {out[key].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_O2RwtWuoVi"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4DjB3B4uoVj"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"codeparrot-ds\",\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=5_000,\n",
        "    logging_steps=5_000,\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.1,\n",
        "    warmup_steps=1_000,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    learning_rate=5e-4,\n",
        "    save_steps=5_000,\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"valid\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6Nm8DjquoVk"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrr3JbxKuoVl"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f9tPPVfuoVm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "pipe = pipeline(\n",
        "    \"text-generation\", model=\"huggingface-course/codeparrot-ds\", device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGGsMh75uoVn",
        "outputId": "53c584a8-766f-405c-c897-2d94cad04b00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "# create some data\n",
              "x = np.random.randn(100)\n",
              "y = np.random.randn(100)\n",
              "\n",
              "# create scatter plot with x, y\n",
              "plt.scatter(x, y)\n",
              "\n",
              "# create scatter"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "txt = \"\"\"\\\n",
        "# create some data\n",
        "x = np.random.randn(100)\n",
        "y = np.random.randn(100)\n",
        "\n",
        "# create scatter plot with x, y\n",
        "\"\"\"\n",
        "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gACocs_auoVo",
        "outputId": "796330fd-a242-420a-b536-5937a9d89f64"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "# create some data\n",
              "x = np.random.randn(100)\n",
              "y = np.random.randn(100)\n",
              "\n",
              "# create dataframe from x and y\n",
              "df = pd.DataFrame({'x': x, 'y': y})\n",
              "df.insert(0,'x', x)\n",
              "for"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "txt = \"\"\"\\\n",
        "# create some data\n",
        "x = np.random.randn(100)\n",
        "y = np.random.randn(100)\n",
        "\n",
        "# create dataframe from x and y\n",
        "\"\"\"\n",
        "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nD4eOnr2uoVp",
        "outputId": "f726bf99-aa92-4626-c2b7-607b3421a188"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "# dataframe with profession, income and name\n",
              "df = pd.DataFrame({'profession': x, 'income':y, 'name': z})\n",
              "\n",
              "# calculate the mean income per profession\n",
              "profession = df.groupby(['profession']).mean()\n",
              "\n",
              "# compute the"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "txt = \"\"\"\\\n",
        "# dataframe with profession, income and name\n",
        "df = pd.DataFrame({'profession': x, 'income':y, 'name': z})\n",
        "\n",
        "# calculate the mean income per profession\n",
        "\"\"\"\n",
        "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxCSYz1zuoVq",
        "outputId": "956df299-373e-4fe5-f310-eed7af01d33d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "# import random forest regressor from scikit-learn\n",
              "from sklearn.ensemble import RandomForestRegressor\n",
              "\n",
              "# fit random forest model with 300 estimators on X, y:\n",
              "rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)\n",
              "rf.fit(X, y)\n",
              "rf"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "txt = \"\"\"\n",
        "# import random forest regressor from scikit-learn\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# fit random forest model with 300 estimators on X, y:\n",
        "\"\"\"\n",
        "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsfT3aQvuoVr",
        "outputId": "bacfe762-1706-4b16-baee-df2cb1cf947b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Keyword has not single token: testtest'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "keytoken_ids = []\n",
        "for keyword in [\n",
        "    \"plt\",\n",
        "    \"pd\",\n",
        "    \"sk\",\n",
        "    \"fit\",\n",
        "    \"predict\",\n",
        "    \" plt\",\n",
        "    \" pd\",\n",
        "    \" sk\",\n",
        "    \" fit\",\n",
        "    \" predict\",\n",
        "    \"testtest\",\n",
        "]:\n",
        "    ids = tokenizer([keyword]).input_ids[0]\n",
        "    if len(ids) == 1:\n",
        "        keytoken_ids.append(ids[0])\n",
        "    else:\n",
        "        print(f\"Keyword has not single token: {keyword}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xi2MD8JmuoVs"
      },
      "outputs": [],
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "import torch\n",
        "\n",
        "\n",
        "def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):\n",
        "    # Shift so that tokens < n predict n\n",
        "    shift_labels = inputs[..., 1:].contiguous()\n",
        "    shift_logits = logits[..., :-1, :].contiguous()\n",
        "    # Calculate per-token loss\n",
        "    loss_fct = CrossEntropyLoss(reduce=False)\n",
        "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "    # Resize and average loss per sample\n",
        "    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)\n",
        "    # Calculate and scale weighting\n",
        "    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(\n",
        "        axis=[0, 2]\n",
        "    )\n",
        "    weights = alpha * (1.0 + weights)\n",
        "    # Calculate weighted average\n",
        "    weighted_loss = (loss_per_sample * weights).mean()\n",
        "    return weighted_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aY9r8bLquoVu"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "tokenized_dataset.set_format(\"torch\")\n",
        "train_dataloader = DataLoader(tokenized_dataset[\"train\"], batch_size=32, shuffle=True)\n",
        "eval_dataloader = DataLoader(tokenized_dataset[\"valid\"], batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAtO7q6iuoVx"
      },
      "outputs": [],
      "source": [
        "weight_decay = 0.1\n",
        "\n",
        "\n",
        "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
        "    params_with_wd, params_without_wd = [], []\n",
        "    for n, p in model.named_parameters():\n",
        "        if any(nd in n for nd in no_decay):\n",
        "            params_without_wd.append(p)\n",
        "        else:\n",
        "            params_with_wd.append(p)\n",
        "    return [\n",
        "        {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n",
        "        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7xl0_FfuoV0"
      },
      "outputs": [],
      "source": [
        "def evaluate():\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    for step, batch in enumerate(eval_dataloader):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
        "\n",
        "        losses.append(accelerator.gather(outputs.loss))\n",
        "    loss = torch.mean(torch.cat(losses))\n",
        "    try:\n",
        "        perplexity = torch.exp(loss)\n",
        "    except OverflowError:\n",
        "        perplexity = float(\"inf\")\n",
        "    return loss.item(), perplexity.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SStqvOduoV2"
      },
      "outputs": [],
      "source": [
        "model = GPT2LMHeadModel(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OjYToYZuoV7"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(get_grouped_params(model), lr=5e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wgkz0FDuoV9"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator = Accelerator(fp16=True)\n",
        "\n",
        "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, eval_dataloader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWSTd01iuoWB"
      },
      "outputs": [],
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_train_epochs = 1\n",
        "num_update_steps_per_epoch = len(train_dataloader)\n",
        "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=1_000,\n",
        "    num_training_steps=num_training_steps,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsLv3DkVuoWC",
        "outputId": "0476a9b6-cd04-4339-86c4-2c285be5399f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'sgugger/codeparrot-ds-accelerate'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from huggingface_hub import Repository, get_full_repo_name\n",
        "\n",
        "model_name = \"codeparrot-ds-accelerate\"\n",
        "repo_name = get_full_repo_name(model_name)\n",
        "repo_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnRDfuokuoWD"
      },
      "outputs": [],
      "source": [
        "output_dir = \"codeparrot-ds-accelerate\"\n",
        "repo = Repository(output_dir, clone_from=repo_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mf_T4jA8uoWD",
        "outputId": "edba33da-12fa-464c-da74-3a885e71ff05"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10.934126853942871, 56057.14453125)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xajzbyJkuoWE"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "gradient_accumulation_steps = 8\n",
        "eval_steps = 5_000\n",
        "\n",
        "model.train()\n",
        "completed_steps = 0\n",
        "for epoch in range(num_train_epochs):\n",
        "    for step, batch in tqdm(\n",
        "        enumerate(train_dataloader, start=1), total=num_training_steps\n",
        "    ):\n",
        "        logits = model(batch[\"input_ids\"]).logits\n",
        "        loss = keytoken_weighted_loss(batch[\"input_ids\"], logits, keytoken_ids)\n",
        "        if step % 100 == 0:\n",
        "            accelerator.print(\n",
        "                {\n",
        "                    \"lr\": get_lr(),\n",
        "                    \"samples\": step * samples_per_step,\n",
        "                    \"steps\": completed_steps,\n",
        "                    \"loss/train\": loss.item() * gradient_accumulation_steps,\n",
        "                }\n",
        "            )\n",
        "        loss = loss / gradient_accumulation_steps\n",
        "        accelerator.backward(loss)\n",
        "        if step % gradient_accumulation_steps == 0:\n",
        "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            completed_steps += 1\n",
        "        if (step % (eval_steps * gradient_accumulation_steps)) == 0:\n",
        "            eval_loss, perplexity = evaluate()\n",
        "            accelerator.print({\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n",
        "            model.train()\n",
        "            accelerator.wait_for_everyone()\n",
        "            unwrapped_model = accelerator.unwrap_model(model)\n",
        "            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
        "            if accelerator.is_main_process:\n",
        "                tokenizer.save_pretrained(output_dir)\n",
        "                repo.push_to_hub(\n",
        "                    commit_message=f\"Training in progress step {step}\", blocking=False\n",
        "                )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Training a causal language model from scratch (PyTorch)",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}