{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjznRhiT6WPr"
      },
      "source": [
        "# Tokenizers (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYqfG5ki6WP4"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Pv2ehWEk6WP6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: no matches found: transformers[sentencepiece]\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lxrlJtY36WP_",
        "outputId": "d984d6c9-4824-40bf-8b97-e6b1aab05511"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Jim', 'Henson', 'was', 'a', 'puppeteer']\n"
          ]
        }
      ],
      "source": [
        "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
        "print(tokenized_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "245vRQJc6WQD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: 100%|██████████| 213k/213k [00:03<00:00, 70.1kB/s] \n",
            "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 5.58kB/s]\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iCIVCLwz6WQF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: 100%|██████████| 436k/436k [00:04<00:00, 92.7kB/s] \n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xKBRoLjJ6WQI",
        "outputId": "48ae4f2c-af48-4f6a-e25d-080bb19fc64f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer(\"Using a Transformer network is simple\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CxyySD_Y6WQL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('tokens/tokenizer_config.json',\n",
              " 'tokens/special_tokens_map.json',\n",
              " 'tokens/vocab.txt',\n",
              " 'tokens/added_tokens.json',\n",
              " 'tokens/tokenizer.json')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.save_pretrained(\"tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5r7DUCRp6WQM",
        "outputId": "68a8615c-ff5b-4f3b-e005-5bd11d42b7e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence = \"Using a Transformer network is simple\"\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mCwu2Uz76WQQ",
        "outputId": "0a7685c5-6d2a-4626-e199-50207164993b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n"
          ]
        }
      ],
      "source": [
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9oVm5MTE6WQT",
        "outputId": "3f7dfdb2-c97d-45f9-9e8b-e25d009706b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using a transformer network is simple\n"
          ]
        }
      ],
      "source": [
        "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
        "print(decoded_string)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Tokenizers (PyTorch)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
